{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Documentation: https://rapidapi.com/fkal094tiokg09w3vi095i/api/Airbnb%20Scraper%20API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before, make sure that the GCP libraries are installed\n",
    "\n",
    "## pip install google\n",
    "## pip install google.cloud\n",
    "## pip install google.cloud.bigquery\n",
    "## pip install google.cloud.storage\n",
    "## pip install datetime, timedelta\n",
    "## pip install pandas\n",
    "## pip install db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries to execute querying on BigQuery\n",
    "\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from credentials import bq_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Credentials from Google Cloud\n",
    "\n",
    "CREDENTIALS = service_account.Credentials.from_service_account_file(bq_path)\n",
    "BIGQUERY = bigquery.Client(credentials=CREDENTIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section aims to create automated variables to perform the scrapping more sustainably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating check-in and check-out variables\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "currentDate =  datetime.today()\n",
    "\n",
    "## Creating list to save CheckIn and Checkout combinations\n",
    "\n",
    "checkInAndOutDates = [\n",
    "        {'checkin': (currentDate + timedelta(days=30)).strftime('%Y-%m-%d'), \n",
    "         'checkout': (currentDate + timedelta(days=60)).strftime('%Y-%m-%d')\n",
    "        },\n",
    "        #{'checkin': (currentDate + timedelta(days=60)).strftime('%Y-%m-%d'), \n",
    "         #'checkout': (currentDate + timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "        #},\n",
    "        ##{'checkin': (currentDate + timedelta(days=90)).strftime('%Y-%m-%d'), \n",
    "        ## 'checkout': (currentDate + timedelta(days=120)).strftime('%Y-%m-%d')\n",
    "        ##}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating location variables \n",
    "## Query collecting Tabas Operating Neighbourhoods\n",
    "\n",
    "sql =  \"\"\"\n",
    "        SELECT \n",
    "            DISTINCT CONCAT(city, ', ', neighborhood) AS city_and_neighbourhood_search\n",
    "        FROM `tabas-dw.master_data.dim_tabas_buildings_and_apartments`\n",
    "        LIMIT 1\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tabas/.local/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Creating dataframe neighbourhoods to write the results\n",
    "\n",
    "neighbourhoods = BIGQUERY.query(sql).result().to_dataframe()\n",
    "neighbourhoods = neighbourhoods.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sections aims to make a GET Request to the Airbnb Scraper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating dataframe df to write the following loop results\n",
    "\n",
    "df = pd.DataFrame(columns=['badges'\n",
    "                           , 'coordinates'\n",
    "                            , 'id'\n",
    "                            , 'images'\n",
    "                            , 'price'\n",
    "                            , 'rating'\n",
    "                            , 'reviews'\n",
    "                            , 'roomTitle'\n",
    "                            , 'roomType'\n",
    "                            , 'subTitle'\n",
    "                            , 'title'\n",
    "                            , 'url'\n",
    "                            , 'location'\n",
    "                            , 'checkin'\n",
    "                            , 'checkout'\n",
    "                            , 'adults'\n",
    "                            , 'scrappedPage'\n",
    "                            , 'extractionTimestamp'\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Request:  {'location': 'São Paulo, Jardins', 'checkIn': '2025-02-22', 'checkOut': '2025-03-24', 'adults': '2', 'roomType': '2', 'cursor': None}\n",
      "Successfully added API request to DataFrame\n",
      "Getting Request:  {'location': 'São Paulo, Jardins', 'checkIn': '2025-02-22', 'checkOut': '2025-03-24', 'adults': '2', 'roomType': '2', 'cursor': 'eyJzZWN0aW9uX29mZnNldCI6MCwiaXRlbXNfb2Zmc2V0IjoxOCwidmVyc2lvbiI6MX0='}\n",
      "Successfully added API request to DataFrame\n",
      "Sucessfully scraped  2  pages\n",
      "End of API request\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "## Define the variables to access Airbnb Scraper API\n",
    "\n",
    "url = \"https://airbnb-scraper-api.p.rapidapi.com/airbnb_search_stays_v2\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"3518f08619msh6dcfedfcbed6577p1d7ff2jsn6219c0aab48e\",\n",
    "    'x-rapidapi-host': \"airbnb-scraper-api.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(len(neighbourhoods)):\n",
    "\n",
    "    for j in range(len(checkInAndOutDates)):\n",
    "\n",
    "        ## The cursor is an unique indicator of the page, this helps the API to know which page to scrap next\n",
    "        ## It is re-set no None on a new request\n",
    "\n",
    "        cursor = None\n",
    "        hasNextPage = True\n",
    "\n",
    "        ## Creating the following dataframe to follow-up the amount of pages scrapped\n",
    "        ## It is re-set to empty on a new request\n",
    "\n",
    "        cursorDataFrame = []\n",
    "\n",
    "        ## The following parameter estipulates the limit amount of pages to be scrapped\n",
    "        ## , if desired\n",
    "\n",
    "        pageLimitation = 2\n",
    "\n",
    "        while hasNextPage and len(cursorDataFrame) < pageLimitation:\n",
    "    \n",
    "            querystring = {\n",
    "                \"location\": neighbourhoods[i][0],                  # Desired location\n",
    "                \"checkIn\": checkInAndOutDates[j]['checkin'],       # Check-in Date\n",
    "                \"checkOut\": checkInAndOutDates[j]['checkout'],      # Check-out Date\n",
    "                \"adults\": \"2\",                                      # Number of adults\n",
    "                \"roomType\": \"2\",                                    # Type of Acommodation: Entire Space\n",
    "                \"cursor\": cursor\n",
    "            }\n",
    "\n",
    "            ## Logging the location being sent to the request\n",
    "\n",
    "            print(\"Getting Request: \", querystring)\n",
    "\n",
    "            ## Send GET request to the API\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "            ## Extract the JSON text data into the variable 'data'\n",
    "\n",
    "            data = response.text\n",
    "\n",
    "            ## Convert JSON into a Pandas Dataframe\n",
    "\n",
    "            data = json.loads(data)\n",
    "            extracted = pd.DataFrame.from_dict(data['data'])\n",
    "\n",
    "            ## Setting the new cursor value to scrape the following page\n",
    "\n",
    "            cursor = data['pageInfo']['endCursor']\n",
    "            hasNextPage = data['pageInfo']['hasNextPage']\n",
    "\n",
    "            ## Add the cursor result to the cursor dataframe\n",
    "\n",
    "            cursorDataFrame.append(cursor)\n",
    "\n",
    "            ## Create new columns on extracted DataFrame to append API variables\n",
    "\n",
    "            extracted['location'] = neighbourhoods[i][0]\n",
    "            extracted['checkin'] = checkInAndOutDates[j]['checkin']\n",
    "            extracted['checkout'] = checkInAndOutDates[j]['checkout']\n",
    "            extracted['adults'] = 2\n",
    "            #extracted['roomType'] = 2     -> Information already exists on JSON\n",
    "            extracted['scrappedPage'] = len(cursorDataFrame) + 1 \n",
    "            extracted['extractionTimestamp'] = datetime.today().strftime('%Y-%m-%d %X')\n",
    "\n",
    "            ## Add the result to the previous created Dataframe\n",
    "            \n",
    "            df = df = pd.concat([df, extracted])\n",
    "\n",
    "            print(\"Successfully added API request to DataFrame\")\n",
    "\n",
    "        else:\n",
    "            if len(cursorDataFrame) > 0:\n",
    "                print(\"Sucessfully scraped \", len(cursorDataFrame), \" pages\")\n",
    "            else: \n",
    "                print(\"Error on API request\")\n",
    "\n",
    "print(\"End of API request\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   badges               17 non-null     object\n",
      " 1   coordinates          18 non-null     object\n",
      " 2   id                   18 non-null     object\n",
      " 3   images               18 non-null     object\n",
      " 4   price                18 non-null     object\n",
      " 5   rating               18 non-null     object\n",
      " 6   reviews              18 non-null     object\n",
      " 7   roomTitle            18 non-null     object\n",
      " 8   roomType             18 non-null     object\n",
      " 9   subTitle             18 non-null     object\n",
      " 10  title                18 non-null     object\n",
      " 11  url                  18 non-null     object\n",
      " 12  location             18 non-null     object\n",
      " 13  checkin              18 non-null     object\n",
      " 14  checkout             18 non-null     object\n",
      " 15  adults               18 non-null     object\n",
      " 16  scrappedPage         18 non-null     object\n",
      " 17  extractionTimestamp  18 non-null     object\n",
      "dtypes: object(18)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focus on the Ingestion of the resulting DataFrame to a specific Datalake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Credentials from Google Cloud\n",
    "\n",
    "from google.cloud import storage\n",
    "STORAGE = storage.Client(credentials=CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessing Bucket Path\n",
    "\n",
    "bucket = STORAGE.get_bucket('us-central1-airflow-v5-30f2abcd-bucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próximos Passos:\n",
    "- Esconder credenciais ou caminho das credenciais\n",
    "- Tratar dataframe\n",
    "- Entender possibilidades de Ingestão deste dataframe\n",
    "- Transformar dataframe em delta table?\n",
    "- Acrescentar biblioteca de log\n",
    "\n",
    "Cálculo de viabilidade:\n",
    "Cenário 1:\n",
    "- Tabas opera em 35 bairros distintos;\n",
    "- 3 requisições distintas por bairro (supondo 3 datas de check-in/check-out)\n",
    "- 6 páginas em média por bairro/check-in/ceck-out\n",
    "- executando 1 vez por semana\n",
    "TOTAL DE REQUISIÇÕES: 2520 \n",
    "\n",
    "Cenário 2:\n",
    "- Tabas opera em 35 bairros distintos;\n",
    "- 4 requisições distintas por bairro (supondo 4 datas de check-in/check-out)\n",
    "- 10 páginas em média por bairro/check-in/ceck-out\n",
    "- executando 1 vez por semana\n",
    "TOTAL DE REQUISIÇÕES: 5600\n",
    "\n",
    "Cenário 3:\n",
    "- Tabas opera em 35 bairros5 distintos;\n",
    "- 5 requisições distintas por bairro (supondo 5 datas de check-in/check-out)\n",
    "- 8 páginas em média por bairro/check-in/ceck-out\n",
    "- executando 6 vezes por mês\n",
    "TOTAL DE REQUISIÇÕES: 8400\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
